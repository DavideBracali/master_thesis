\pagestyle{fancy}
\fancyhead{}

\fancyheadoffset{0cm}
\renewcommand{\headrulewidth}{1pt} 
\renewcommand{\footrulewidth}{0pt}
\fancyhead[L]{{\thesection}}
\fancyhead[L]{\leftmark}

% \fancyhead[LO,LE]{\nouppercase\firstleftmark}
%\fancyhead[RO,RE]{\thesubsection\enspace\subsectiontitle}

\section{Results and Discussion}    

\subsection{Analysis on a subset of patients}   \label{res:anal}

In this section, we present a detailed analysis of the seizure dynamics on a
subset of 4 patients.
This analysis was performed in order mainly for three reasons:
\begin{itemize}
    \item To define the most discriminative local features to differentiate
    between contacts that were labeled as SOZ and healthy regions of the brain;
    \item To understand the similarities across different patients in the global
    dynamics of a seizure;
    \item To illustrate a classification example for one representative
    patient in order to interpret intermediate results in the pipeline.
\end{itemize}

Because this investigation was performed preliminarily and defined design
choices in the model pipeline, we decided to exclude these 4 patients from the 
model evaluation to prevent any potential information leakage that would lead to
an overestimation of the model performances.
They appear as part of the training set in the hyperparameter optimization, but
are never included in neither validation and test set.

\subsubsection{Feature importance}  \label{res:anal:importance}

This section aims to define the most important local features in discriminating
between SOZ and healthy brain regions during the onset of a feature.
A two-sided Mann-Whitney test was performed on the distributions of each of the 
22 local features during the onset of the seizure, defined as a $[t_\text{onset}
-10\ s, t_\text{onset} + 20\ s]$ interval.
Both rank biserial correlation (whose definition is reported in Section
\ref{meth:opt:vaegmm}) and p-value are reported, to measure effect size and
statistical significance.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Immagini/feature_effect_size.png}
    \caption{Discriminative power of local features in a subset of 4 patients.}
    \label{fig:effect_size}
\end{figure}

Figure \ref{fig:effect_size} displays how betweenness centrality has both the 
larges effect size and the only significant p-value.
Equivalently, it is the only local feature whose distribution in the seizure
onset is significantly different between SOZ and non-SOZ labeled contacts.

This result directly influences the architecture of the alarm classifier
described in Section \ref{meth:classifier}, as the negative log-likelihood is 
weighted according to the absolute value of the Z-score of betweenness
centrality.
However, the proposed model does not solely rely on betweenness centrality, but
considers a negative log-likelihood score that reflects the anomalies 
across the joint distribution of all different features.
Betweenness centrality is only used, together with the temporal index, as a 
weighting factor for anomalies, amplifying those occurring when the most
discriminative feature shows strong deviations.

\subsubsection{Global seizure dynamics}     \label{res:anal:global}

This section displays and analyzes the global feature dynamics during a seizure.
To avoid overloading the reader with an excessive amount of figures, only one
representative patient will be shown.
However, every pattern reported in this section is observable in all the 4
patients included in the analysis.

\subsubsection{A classification example}    \label{res:anal:example}

!!! Far vedere i plot dello spazio latente, della likelihood e 
grafico a barre del segnale d'allarme spiegando cosa succede

\subsection{Optimization Results}       \label{res:opt}

Hyperparameter optimization was performed as described in Section \ref{meth:opt}.
In this section we will present the final parameters,
the distribution of the best parameters for the cross-validation,
the learning curves, and the distributions of the optimization target for
training, validation and test set.
The results presented in this section will be discussed in Section
\ref{res:discussion}.

\subsubsection{VAE and GMM}         \label{res:opt:vaegmm}

Optimization of the VAE and GMM architecture was performed for 300 iterations on
each LOOCV fold.
As discussed in Section \ref{meth:opt} the validation target\footnote{
Rank biserial correlation of one-sided Mann-Whitney test comparing the negative
log-likelihood of SOZ and non-SOZ contacts during a seizure. Refer to Section
\ref{meth:opt:vaegmm} for details.}
on each fold was monitored during training.
The median validation target stopped increasing after iteration 193
(!!! valutare se inserire immagine),
for this reason the final training before evalutating test patients was limited
to 193 iterations.

Figure \ref{fig:opt1_params} shows the best parameters for each LOOCV fold,
while Table \ref{tab:opt1_params} presents the best parameters returned by the
final training procedure.
The learning process of the final training is shown in Figure
\ref{fig:opt1_learn}, while Figure \ref{fig:opt1_target} displays the 
target distribution on training, validation and test set.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Immagini/opt/best_params_1.png}
    \caption{Best parameters in cross-validation for VAE and GMM optimization.}
    \label{fig:opt1_params}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{Immagini/opt/optimization_curve_1.png}
    \caption{Learning curve in the final training of VAE and GMM optimization.}
    \label{fig:opt1_learn}
\end{figure}

\begin{table}[h]
\centering
\begin{tabular}{lcc}
\hline
\textbf{Parameter} & \textbf{Best value} \\
\hline
$\beta_\text{max}$     & 1  \\
$\beta_\text{warmup}$  & 10 \\
$d$                    & 2  \\
$\eta$                 & 0.024 \\
$L$                    & 0 \\
$N_c$                  & 2 \\
\hline
\end{tabular}
\caption{Best parameters for the final training of VAE and GMM optimization.}
\label{tab:opt1_params}
\end{table}

\begin{figure}[H]
    \centering
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Immagini/opt/contrast_distributions_val.png}
        \caption{Cross-validation}
    \end{subfigure}
    \hfill
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Immagini/opt/contrast_distributions_test.png}
        \caption{Final training and test}
    \end{subfigure}
    \caption{Training, validation and test distributions of optimization target 
    for VAE and GMM optimization.}
    \label{fig:opt1_target}
\end{figure}


\subsubsection{Classifier}      \label{res:opt:classifier}

Optimization of the alarm classifier was performed for 200 iterations on
each LOOCV fold.
The median validation target\footnote{
Arithmetic average of precision (PPV) and specificity (SPC) multiplied by a
penalty function. Refer to Section \ref{meth:opt:classifier} for details.}
stopped increasing after iteration 173,
(!!! valutare se inserire immagine),
therefore the final training was limited to 173 iterations.

Figure \ref{fig:opt2_params} shows the best parameters for each LOOCV fold,
while Table \ref{tab:opt2_params} presents the best parameters returned by the
final training procedure.
The learning process of the final training is shown in Figure
\ref{fig:opt2_learn}, while Figure \ref{fig:opt2_target} displays the 
target distribution on training, validation and test set.
Because the target for this phase of optimization is itself a binary evaluation
metric, Figure \ref{fig:opt2_target} also reports a comparison with the random
classifier and the automated version of the Epileptogenicity Index proposed by 
Bartolomei et al. \cite{bartolomei}.


\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Immagini/opt/best_params_2.png}
    \caption{Best parameters in cross-validation for the classifier optimization.}
    \label{fig:opt2_params}
\end{figure}


\begin{table}[H]
\centering
\begin{tabular}{lcc}
\hline
\textbf{Parameter} & \textbf{Best value} \\
\hline
$\alpha$               & 0.96  \\
$k$                    & -2.3 \\
$t_C$                  & 1.8  \\
$\tau$                 & 1.2 \\
\hline
\end{tabular}
\caption{Best parameters for the final training of the classifier optimization.}
\label{tab:opt2_params}
\end{table}


\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{Immagini/opt/optimization_curve_2.png}
    \caption{Learning curve in the final training of the classifier optimization.}
    \label{fig:opt2_learn}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.9\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Immagini/opt/score_distributions_val.png}
        \caption{Cross-validation}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.9\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Immagini/opt/score_distributions_test.png}
        \caption{Final training and test}
    \end{subfigure}
    \caption{Training, validation and test distributions of optimization target 
    for the classifier optimization.}
    \label{fig:opt2_target}
\end{figure}


\subsection{Evaluation Metrics}     \label{res:eval}

This section reports precision (Figure \ref{fig:prec}), specificity (Figure
\ref{fig:spec}), balanced accuracy (Figure \ref{fig:ba}) and ROC-AUC
(Figure \ref{fig:auc}) for training, validation and test patients, 
while comparing such distributions with
the results that would obtain a random classifier and the automated version of
the Epileptogenicity Index proposed by Bartolomei et al. \cite{bartolomei}.

Table \ref{tab:eval} summarizes the evaluation results.
It is important to remind, as explained in Section \ref{meth:opt:classifier},
that the ground truth may label as positive contacts that do not show
epileptogenic behavior in the single seizure that is available for each patient.
For this reason we consider precision and specificity as meaningful statistics
as they do not depend on the number of False Positives.
ROC-AUC and balanced accuracy are reported anyway, however their distributions
may be a conservative estimate of the actual discriminative capabilities of the
model.

!!! Tabella

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.9\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Immagini/eval_val_set/prec_distributions.png}
        \caption{Cross-validation}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.9\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Immagini/eval_test_set/prec_distributions.png}
        \caption{Final training and test}
    \end{subfigure}
    \caption{Precision (PPV) distributions in training, validation and test set.}
    \label{fig:prec}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.9\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Immagini/eval_val_set/spec_distributions.png}
        \caption{Cross-validation}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.9\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Immagini/eval_test_set/spec_distributions.png}
        \caption{Final training and test}
    \end{subfigure}
    \caption{Specificity (SPC) distributions in training, validation and test set.}
    \label{fig:spec}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.9\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Immagini/eval_val_set/ba_distributions.png}
        \caption{Cross-validation}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.9\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Immagini/eval_test_set/ba_distributions.png}
        \caption{Final training and test}
    \end{subfigure}
    \caption{Balanced Accuracy (BA) distributions in training, validation and test set.}
    \label{fig:ba}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.9\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Immagini/eval_val_set/auc_distributions.png}
        \caption{Cross-validation}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.9\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Immagini/eval_test_set/auc_distributions.png}
        \caption{Final training and test}
    \end{subfigure}
    \caption{ROC-AUC distributions in training, validation and test set.}
    \label{fig:auc}
\end{figure}


\subsection{Discussion}         \label{res:discussion}

Although promising results were obtained during cross-validation, it is evident 
that the proposed model failed to generalize to a subset of unseen patients:
Figure \ref{fig:prec} and Figure \ref{fig:spec} show that the 
model obtained worse precision and specificity distributions on test patients
compared to those observed in the training and validation sets.
While all validation metrics distributions significantly differ from the ones
produced by the random classifier, the same cannot be said for test
distributions\footnote{
Given the limited number of independent test patients ($N = 4$), the statistical 
power of the Mannâ€“Whitney test is inherently low. Therefore, the lack of 
statistical significance should not be interpreted as evidence of equivalent
performance.
Nevertheless, the performance drop observed on the test set shows that the model
struggles to generalize to new patients and is probably overfitting the training
patients.}.

This result is likely to be a consequence of two factors: the combination of 
a limited number of patients and a great heterogeinity in seizure and resting
-state dynamics. (!!! valutare se inserire il discorso degli spazi latenti magari
con immagine colorata)
In the LOOCV setting, the selected hyperparameters are highly sensitive to the
specific training fold, as
the replacement of a single patient in the training set leads to substantially
different optimal hyperparameters (Figure \ref{fig:opt1_params} and Figure
\ref{fig:opt2_params}).
As a consequence, training target distributions during optimization present
very high variance (Figure \ref{fig:opt1_target} and Figure
\ref{fig:opt2_target}), as the model struggles to
find an unique set of parameters to obtain acceptable performances on all
patients.

It is also worth noticing the instability of the VAE and GMM optimization
learning curve in Figure \ref{fig:opt1_learn}, indicative that the model
performance is highly sensitive on small variations of the parameters.
Together with the fold-dependent sensitivity observed in LOOCV, this further
explains the limited generalization capabilities on unseen patients.

Validation performances, on the other hand, appear comparable with the training
set.
However it should be noted that the number of training iterations before 
evaluation of the test set was selected based on the validation performance
itself.
Because of the diversity between validation and test set, validation
performances represents an optimistic estimate of the real generalization
capabilities of the model.

The automated version of Bartolomei et al. Epileptogenicity Index was proven
incapable to return acceptable classification performances on both validation 
and test patients.
Its low precision (Figure \ref{fig:prec}) and high specificity (Figure 
\ref{fig:spec}) values suggest that the model only classifies a few contacts as 
positive, and the majority of predicted positives is usually labeled as negative.
This proves that Bartolomei et al. model cannot replicate the performances 
reported in the original paper when generalized to multiple patients using an
unique set of parameters.


\subsection{Future developments}              \label{res:discussion:future}

!!! Idee su come migliorare: fine tuning, VAE supervisionato, provare altri
modelli, classificatore deep learning