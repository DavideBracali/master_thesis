\pagestyle{fancy}
\fancyhead{}

\fancyheadoffset{0cm}
\renewcommand{\headrulewidth}{1pt} 
\renewcommand{\footrulewidth}{0pt}
\fancyhead[L]{{\thesection}}
\fancyhead[L]{\leftmark}

% \fancyhead[LO,LE]{\nouppercase\firstleftmark}
%\fancyhead[RO,RE]{\thesubsection\enspace\subsectiontitle}

\section{Results and Discussion}    

\subsection{Analysis on a subset of patients}   \label{res:anal}

!!! Mostrare un paziente (se no diventa troppo lunga come immagini) come in
presentazione mettendo gli altri 3 in appendice

\subsubsection{Feature importance}  \label{res:anal:importance}

!!! Grafico a barre delle features, giustificare la scelta della betweenness come
peso per il classificatore

\subsubsection{Global seizure dynamics}     \label{res:anal:global}

!!! Mostrare tutte e 22 le features, spiegando i trend più evidenti

\subsubsection{A classification example}    \label{res:anal:example}

!!! Far vedere i plot dello spazio latente, della likelihood e 
grafico a barre del segnale d'allarme spiegando cosa succede

\subsection{Optimization Results}       \label{res:opt}

Hyperparameter optimization was performed as described in Section \ref{meth:opt}.
In this section we will present the final parameters,
the distribution of the best parameters for the cross-validation,
the learning curves, and the distributions of the optimization target for
training, validation and test set.
The results presented in this section will be discussed in Section
\ref{res:discussion}.

\subsubsection{VAE and GMM}         \label{res:opt:vaegmm}

Optimization of the VAE and GMM architecture was performed for 300 iterations on
each LOOCV fold.
As discussed in Section \ref{meth:opt} the validation target\footnote{
Rank biserial correlation of one-sided Mann-Whitney test comparing the negative
log-likelihood of SOZ and non-SOZ contacts during a seizure. Refer to Section
\ref{meth:opt:vaegmm} for details.}
on each fold was monitored during training.
The median validation target stopped increasing after iteration 193
(!!! valutare se inserire immagine),
for this reason the final training before evalutating test patients was limited
to 193 iterations.

Figure \ref{fig:opt1_params} shows the best parameters for each LOOCV fold,
while Table \ref{tab:opt1_params} presents the best parameters returned by the
final training procedure.
The learning process of the final training is shown in Figure
\ref{fig:opt1_learn}, while Figure \ref{fig:opt1_target} displays the 
target distribution on training, validation and test set.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Immagini/opt/best_params_1.png}
    \caption{Best parameters in cross-validation for VAE and GMM optimization.}
    \label{fig:opt1_params}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{Immagini/opt/optimization_curve_1.png}
    \caption{Learning curve in the final training of VAE and GMM optimization.}
    \label{fig:opt1_learn}
\end{figure}

\begin{table}[h]
\centering
\begin{tabular}{lcc}
\hline
\textbf{Parameter} & \textbf{Best value} \\
\hline
$\beta_\text{max}$     & 1  \\
$\beta_\text{warmup}$  & 10 \\
$d$                    & 2  \\
$\eta$                 & 0.024 \\
$L$                    & 0 \\
$N_c$                  & 2 \\
\hline
\end{tabular}
\caption{Best parameters for the final training of VAE and GMM optimization.}
\label{tab:opt1_params}
\end{table}

\begin{figure}[H]
    \centering
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Immagini/opt/contrast_distributions_val.png}
        \caption{Cross-validation}
    \end{subfigure}
    \hfill
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Immagini/opt/contrast_distributions_test.png}
        \caption{Final training and test}
    \end{subfigure}
    \caption{Training, validation and test distributions of optimization target 
    for VAE and GMM optimization.}
    \label{fig:opt1_target}
\end{figure}


\subsubsection{Classifier}      \label{res:opt:classifier}

Optimization of the alarm classifier was performed for 200 iterations on
each LOOCV fold.
The median validation target\footnote{
Arithmetic average of precision (PPV) and specificity (SPC) multiplied by a
penalty function. Refer to Section \ref{meth:opt:classifier} for details.}
stopped increasing after iteration 173,
(!!! valutare se inserire immagine),
therefore the final training was limited to 173 iterations.

Figure \ref{fig:opt2_params} shows the best parameters for each LOOCV fold,
while Table \ref{tab:opt2_params} presents the best parameters returned by the
final training procedure.
The learning process of the final training is shown in Figure
\ref{fig:opt2_learn}, while Figure \ref{fig:opt2_target} displays the 
target distribution on training, validation and test set.
Because the target for this phase of optimization is itself a binary evaluation
metric, Figure \ref{fig:opt2_target} also reports a comparison with the random
classifier and the automated version of the Epileptogenicity Index proposed by 
Bartolomei et al. \cite{bartolomei}.


\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Immagini/opt/best_params_2.png}
    \caption{Best parameters in cross-validation for the classifier optimization.}
    \label{fig:opt2_params}
\end{figure}


\begin{table}[H]
\centering
\begin{tabular}{lcc}
\hline
\textbf{Parameter} & \textbf{Best value} \\
\hline
$\alpha$               & 0.96  \\
$k$                    & -2.3 \\
$t_C$                  & 1.8  \\
$\tau$                 & 1.2 \\
\hline
\end{tabular}
\caption{Best parameters for the final training of the classifier optimization.}
\label{tab:opt2_params}
\end{table}


\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{Immagini/opt/optimization_curve_2.png}
    \caption{Learning curve in the final training of the classifier optimization.}
    \label{fig:opt2_learn}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.9\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Immagini/opt/score_distributions_val.png}
        \caption{Cross-validation}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.9\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Immagini/opt/score_distributions_test.png}
        \caption{Final training and test}
    \end{subfigure}
    \caption{Training, validation and test distributions of optimization target 
    for the classifier optimization.}
    \label{fig:opt2_target}
\end{figure}


\subsection{Evaluation Metrics}     \label{res:eval}

This section reports precision (Figure \ref{fig:prec}), specificity (Figure
\ref{fig:spec}), balanced accuracy (Figure \ref{fig:ba}) and ROC-AUC
(Figure \ref{fig:auc}) for training, validation and test patients, 
while comparing such distributions with
the results that would obtain a random classifier and the automated version of
the Epileptogenicity Index proposed by Bartolomei et al. \cite{bartolomei}.

Table \ref{tab:eval} summarizes the evaluation results.
It is important to remind, as explained in Section \ref{meth:opt:classifier},
that the ground truth may label as positive contacts that do not show
epileptogenic behavior in the single seizure that is available for each patient.
For this reason we consider precision and specificity as meaningful statistics
as they do not depend on the number of False Positives.
ROC-AUC and balanced accuracy are reported anyway, however their distributions
may be a conservative estimate of the actual discriminative capabilities of the
model.

!!! Tabella

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.9\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Immagini/eval_val_set/prec_distributions.png}
        \caption{Cross-validation}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.9\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Immagini/eval_test_set/prec_distributions.png}
        \caption{Final training and test}
    \end{subfigure}
    \caption{Precision (PPV) distributions in training, validation and test set.}
    \label{fig:prec}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.9\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Immagini/eval_val_set/spec_distributions.png}
        \caption{Cross-validation}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.9\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Immagini/eval_test_set/spec_distributions.png}
        \caption{Final training and test}
    \end{subfigure}
    \caption{Specificity (SPC) distributions in training, validation and test set.}
    \label{fig:spec}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.9\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Immagini/eval_val_set/ba_distributions.png}
        \caption{Cross-validation}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.9\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Immagini/eval_test_set/ba_distributions.png}
        \caption{Final training and test}
    \end{subfigure}
    \caption{Balanced Accuracy (BA) distributions in training, validation and test set.}
    \label{fig:ba}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.9\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Immagini/eval_val_set/auc_distributions.png}
        \caption{Cross-validation}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.9\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Immagini/eval_test_set/auc_distributions.png}
        \caption{Final training and test}
    \end{subfigure}
    \caption{ROC-AUC distributions in training, validation and test set.}
    \label{fig:auc}
\end{figure}


\subsection{Discussion}         \label{res:discussion}

Although promising results were obtained during cross-validation, it is evident 
that the proposed model failed to generalize to a subset of unseen patients:
Figure \ref{fig:prec} and Figure \ref{fig:spec} show that the 
model obtained worse precision and specificity distributions on test patients
compared to those observed in the training and validation sets.
While all validation metrics distributions significantly differ from the ones
produced by the random classifier, the same cannot be said for test
distributions\footnote{
Given the limited number of independent test patients ($N = 4$), the statistical 
power of the Mann–Whitney test is inherently low. Therefore, the lack of 
statistical significance should not be interpreted as evidence of equivalent
performance.
Nevertheless, the performance drop observed on the test set shows that the model
struggles to generalize to new patients and is probably overfitting the training
patients.}.

This result is likely to be a consequence of two factors: the combination of 
a limited number of patients and a great heterogeinity in seizure and resting
-state dynamics. (!!! valutare se inserire il discorso degli spazi latenti magari
con immagine colorata)
In the LOOCV setting, the selected hyperparameters are highly sensitive to the
specific training fold, as
the replacement of a single patient in the training set leads to substantially
different optimal hyperparameters (Figure \ref{fig:opt1_params} and Figure
\ref{fig:opt2_params}).
As a consequence, training target distributions during optimization present
very high variance (Figure \ref{fig:opt1_target} and Figure
\ref{fig:opt2_target}), as the model struggles to
find an unique set of parameters to obtain acceptable performances on all
patients.

It is also worth noticing the instability of the VAE and GMM optimization
learning curve in Figure \ref{fig:opt1_learn}, indicative that the model
performance is highly sensitive on small variations of the parameters.
Together with the fold-dependent sensitivity observed in LOOCV, this further
explains the limited generalization capabilities on unseen patients.

Validation performances, on the other hand, appear comparable with the training
set.
However it should be noted that the number of training iterations before 
evaluation of the test set was selected based on the validation performance
itself.
Because of the diversity between validation and test set, validation
performances represents an optimistic estimate of the real generalization
capabilities of the model.

The automated version of Bartolomei et al. Epileptogenicity Index was proven
incapable to return acceptable classification performances on both validation 
and test patients. !! continua






!!!Confronto con random classifier e Bartolomei



\subsection{Future developments}              \label{res:discussion:future}

!!! Idee su come migliorare: fine tuning, VAE supervisionato, provare altri
modelli, classificatore deep learning