\pagestyle{fancy}
\fancyhead{}

\fancyheadoffset{0cm}
\renewcommand{\headrulewidth}{1pt} 
\renewcommand{\footrulewidth}{0pt}
\fancyhead[L]{{\thesection}}
\fancyhead[L]{\leftmark}

% \fancyhead[LO,LE]{\nouppercase\firstleftmark}
%\fancyhead[RO,RE]{\thesubsection\enspace\subsectiontitle}

\section{Methods and Materials}


\subsection{Dataset Description}

The dataset for this project includes sEEG recordings for 23 patients affected
by focal epilepsy.
Each patient provides one recording containing a seizure event (approximately 3
minutes) and at least 2 hours of resting state recordings, which we will refer 
as "wakes".
For 18 patients two wakes were available, while the other 5 patients only
included one wake.

Each recording consists of several time series, one for each contact (typically
150-200), sampled at 500 Hz.
Each contact is labeled as a letter, representing the electrode, followed by an
integer number referring to a segment of that electrode.
For example, contact "A1" represents the first segment of electrode "A".

(!!! foto raw data??)


Beside the recordings a database was also provided, including the list of the
surgically removed brain regions.
All of the patients in this study underwent surgery, consisting in the 
thermocoagulation of the tissue surrounding the contacts labeled as SOZ.
For the majority of patients, the removal of these regions resulted in a drastic 
reduction of the symptoms of epilepsy. We will therefore consider this list as
the ground truth for this project.
The onset time of the seizure, reported by doctors and corresponding to the
first electrical symptoms, was also included in the database.


\subsection{Pre-processing}

Pre-processing for this data consists in the application of notch filters to 
eliminate power line noise, and a bandpass Butterworth filter.

The notch filters were applied at $50\ Hz$, $100\ Hz$, $150\ Hz$ and $200\ Hz$
to remove the fundamental frequency and its fundamental harmonics
A bandpass Butterworth 6th order zero-phase filter was applied on raw data to
isolate frequencies between $0.5\ Hz$, as very low frequencies
provide no useful information for this study \cite{eeg-book} and $249.5\ Hz$,
as frequencies higher than $250\ Hz$ would introduce aliasing in a $500\ Hz$ 
time series.

To lighten disk usage and computational time, signal was converted from $V$ to  
$\mu V$ and stored to \textit{float32}.


\subsection{The Brain Network} \label{meth:thebrainnetwork}

Network analysis was extensively used in this work: sEEG recordings are
localized, and provide the opportunity to investigate the functional
relationships between different regions of the brain.
Each brain region can be mapped into a node, while an appropriate similarity
measure between the recordings can be used to form weighted links.
Because sEEG recordinggs are time series, such network will also be dynamic,
enabling us to gain insights on the evolution of the network before, during and
after the seizure.

This approach is widely explored in literature \cite{eeg-book}, as epileptic
seizures are characterized by anomalous and synchronized electrical activity
\cite{WHO2016Epilepsy}.
In particular, in focal epileptic seizures the abnormal activity is initially
limited between the SOZ, and subsequently spreads to the entirity (or a vast
area) of the brain.
Functional brain networks are powerful tools to model such synchronization and
diffusion mechanisms.

Different choices are possible for the similarity measure, depending on the
focus of the study \cite{eeg-book}.
For this work mutual information was chosen to quantify the dependencies of
signal pairs, capturing linear and non-linear relationships to model the
dynamics of an epileptic seizure. Mutual information between two discrete
random variables $X\in\mathcal{X}$ and $Y\in\mathcal{Y}$ is defined as:

\begin{equation}
    \text{MI}(X;Y) = D_{KL}(P_{(X,Y)}\ \Vert\ P_X \otimes P_Y)
    = \sum_{x\in\mathcal(X)}\sum_{y\in\mathcal(Y)}
    {P_{(X,Y)}(x,y)\log{\frac{P_{(X,Y)}(x,y)}{P_{X}(x)\cdot P_{Y}(y)}}}
\end{equation}
where $D_{KL}$ is the Kullback–Leibler divergence, a statistical distance
between the joint distribution $(P_{(X,Y)}$ and the outer product of the
marginal distributions $P_X$ and $P_Y$. 
It is a non-negative and symmetric quantity that measures how much the joint
distribution of $(X,Y)$ differs from the product of the marginal distributions
of $X$ and $Y$ \cite{cover1991elements}. Hence, when two random variables
are independent, their mutual information is equal to zero.

Being a metric that quantifies the amount of shared information between two
variables, mutual information can be used to capture the linear and non-linear
relationships that model the synchronization dynamics typical of epilepsy.
For this reason, each sEEG channel was divided into $1\ s$ (or equivalently 500
time steps) non-overlapping time windows.
At a given instant, these segments represent the nodes of fully-connected,
weighted, undirected graph network.
The weight $w_{ij}$ of the link between two nodes $(i,j)$ is defined as:
\begin{equation}
    w_{ij}(t) = \frac{\text{MI}_{ij}(t)}{\text{MI}_{ii}}
\end{equation}
where $\text{MI}_{ij}(t)$ is the mutual information between signal i and j at time $t$.
It is normalized over the mutual information between identical signals $\text{MI}_{ii}$
so that weights always are between 0 and 1,
with $w_{ij} = 0$ indicating statistical
independence and $w_{ij} = 1$ corresponding to identical signals.

This process results in a dynamic set of networks with a temporal resolution of
$1\ s$. Higher temporal resolution could be possible, for example using a
sliding window instead of a non-overlapping one.
One must however keep in mind that, for each time frame, a different network has
to be constructed. The computation time to build and extract features will
therefore scale linearly with the number of time frames.


\subsection{Proposed Pipeline}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Immagini/pipeline.png}
    \caption{Proposed pipeline.}
    \label{fig:pipeline}
\end{figure}

The aim of this work is to develop an patient-independent algorithm to classify
which regions of the brain belong to the SOZ.
The proposed pipeline for this task is shown in figure \ref{fig:pipeline}), and
can be summarized in the following steps:
\begin{itemize}
    \item Feature extraction: a set of 22 metrics is computed from pre-processed
    data, in order to extract meaningful features that characterize sEEG data.
    Details are provided in Section~\ref{meth:features}.
    \item Variational Autoencoder (VAE): features are used as input of a 
    VAE, which encodes data in a lower-dimensional representation.
    The model is trained on wakes to learn an encoded representation
    for the resting state of the brain.
    Details are provided in Section~\ref{meth:vae}
    \item Gaussian Mixture Model (GMM): a weighted sum of normal distributions
    is fitted on the latent space of the VAE, to learn the encoded
    distribution of features during resting state activity.
    Details are provided in Section~\ref{meth:gmm}
    \item Negative log-likelihood score: seizure data are projected into the
    trained latent space, where deviations from the learned resting-state
    distribution result in low likelihood values.
    The negative log-likelihood is computed at each time step and is considered
    as a measure of anomalous behavior.
    Details are provided in Section~\ref{meth:neg_ll}
    \item SOZ classification: we assume that the SOZ is constituted by 
    the contacts that begin to show anomalies a few seconds before the 
    generalized seizure. We use a deterministic algorithm on the negative 
    log-likelihood score to rank contacts based on an alarm score, proportional
    to the estimated probability of belonging to the SOZ.
    Finally, the computed alarm score is thresholded to obtain a binary 
    classification.
    Details are provided in Section~\ref{meth:classifier}
\end{itemize}


\subsection{Feature Extraction} \label{meth:features}

In order to distinguish between healthy and epileptogenic behavior in sEEG
recordings, a set of features must be computed at each time frame in order to
extract meaningful information from data.
In this study we extract a total of 22 features, which can be grouped
into three categories depending on their nature: network features, signal
features and spectral features:
\begin{enumerate}
    \item Network features: they are computed from a graph functional network to
    explore pair-wise relationships between signals.
    \item Signal features: metrics that can be extracted directly from the sEEG
    pre-processed time series.
    \item Spectral features: metrics that characterize the short-time Fourier
    transform specrogram of data.
\end{enumerate}
All metrics were extracted with a $1\ s$ temporal resolution, using
non-overlapping sliding windows for feature extractions.



\subsubsection{Network features}

Network features are metrics that are computed from the functional network
described in section \ref{meth:thebrainnetwork}.
Those quantities can both describe the global state of the brain network
(global features) and inspect the topological role of each individual node
within the network (local features).

Local metrics can provide insights on the individual role of each node in the
network.
Their main purpose in this study is to localize the portions of the brain who
play an important role during the seizure onset.
Global features and their evolution, on the other hand, can be important
to understand and analyze network dynamics before, during and after a seizure.

The following local measures were computed:
\begin{itemize}
    \item Strength: it's defined as the sum of the weights of the links
    associated with each node \cite{networks-weighted}.
    Formally strength of node $i$ reads as:
    \begin{equation}
        s_i = \sum_{j\neq i}{w_{ij}}
    \end{equation}
    Its distribution, and its statistical moments, can provide insights
    about the evolution of the network connectivity.
    High weight indicates that contact $i$ is, on average, high
    synchronization with all the other contacts.

    \item Clustering coefficient: for unweighted graphs, the clustering of a
    node is the fraction of possible triangles through that node that exist 
    \cite{networks-clustering2}.
    For weighted graphs, there are several ways to define clustering, and the
    one used here is defined as the geometric average of the triangle edge
    weights \cite{networks-clustering}.
    Intuitively, it quantifies the tendency of nodes to form highly
    interconnected communities in the network.

    \item Betweenness centrality: it is defined as the sum of the fraction of
    all shortest paths that pass through that node \cite{networks-betw}
    Formally betweenness
    centrality of node $i$ reads as:

     
    \begin{equation}
        g_i = \sum_{i\neq j\neq k}{\frac{\sigma_{jk}(i)}{\sigma_{jk}}}
    \end{equation}
    where $\sigma_{jk}$ is the number of shortest paths from node $j$ to node $k$
    and $\sigma_{jk}(i)$ is the number of shortest paths from node $j$ to node $k$
    that pass through node $i$.
    It is an important centrality measure that highlights nodes acting as
    bridges between different parts of the network.

    \item Eigenvector centrality: it measures the influence of a node in a
    network by considering not only the number of its direct connections but
    also the centrality of the nodes to which it is connected.
    This measure is computed from the eigenvector corresponding to the largest
    eigenvalue of the weighted adjacency matrix \cite{networks-eigen}:
    \begin{equation}
        W\mathbf{x} = \lambda_1\mathbf{x}
    \end{equation}
    where $W$ is the weighted adjacency matrix, $\lambda_1$ is its largest
    eigenvalue and $\mathbf{x}$ contains the eigenvector centrality of each node.
\end{itemize}

The global metrics used for this work are:
\begin{itemize}
    \item Assortativity: it measures the similarity of connections in the graph
    with respect to node strength.
    A positive value (assortative network) indicates that nodes attach
    preferentially to nodes with similar strength, while a negative value
    (disassortative network) corresponds to the opposite tendency
    \cite{networks-ass}.
    
    \item Average shortest path: the sum of shoretest path lengths between all
    pairs of nodes:
    \begin{equation}
        a = \sum_{i\neq j}{\frac{d(i,j)}{n(n-1)}}
    \end{equation}
    where $d(i,j)$ is the shortest path distance between node $i$ and node $j$
    and $n$ is the total number of nodes.
    It can be generalized for weighted networks, using the inverse of the link
    weight as a distance between two nodes (so that higher weights are
    associated to shorter distances).

    \item The average along nodes of all the previously described local metrics.

\end{itemize}


\subsubsection{Signal features}

Signal features can be computed directly from the pre-processed sEEG time-seires.
Such features were choosen to capture epileptogenic patterns that are directly
visible just looking at data, such as increased amplitude, sharp oscillations, 
or sudden transient spikes.

The following metrics were computed for each contact:
\begin{itemize}
    \item Mean: the arithmetic mean of the distribution of signal amplitude.
    
    \item Standard deviation: standard deviation of the distribution of signal
    amplitude, to capture increased signal variability during epileptogenic
    behavior.

    \item Skewness: measure of asymmetry the distribution of signal amplitude.
    It highlights deviations from symmetric signal activity.
    Has shown discriminative power in magnetoencephalography signals
    \cite{skewkurt}.

    \item Kurtosis coefficient: measures the tendency of the signal to produce
    extreme values, using a normal distribution as refernce.
    High Kurtosis indicates frequent large excursions from the mean, which
    correspond to sharp transients and spikes commonly seen in epilepsy.
    Has shown discriminative power in magnetoencephalography signals
    \cite{skewkurt}.
    
    \item RMS (Root Mean Square): measures the overall signal power, equivalent 
    of the standard deviation assuming zero as mean.

    \item Peak-to-peak: maximum excursion between maximum and minimum.
    This metric is particularly sensitive to sudden spikes or sharp waves,
    indicators of epileptogenic activity.
     
    \item Maximum derivative: maximum absolute value of the difference between
    adjacent signal samples, represents the maximum steepness of the signal.
    
    \item Highuci fractal dimension: approximate box-counting dimension of the
    fractal that underlines the time-seires.
    Captures signal complexity and self-similarity, and can be a powerful
    discriminant in identifying seizure onset \cite{higuchi}.
\end{itemize}


\subsubsection{Spectral features}

Spectral features are derived from the spectrogram of pre-processed signals.
The spectrogram, a time-dependent representation of the frequency domain of
signal, was computed using a short-time Fourier transform.
It consists in the sequencing of data using $1\ s $ non-overlapping time windows
and the application of a Fourier transform on those windows, providing a dynamic
analysis of the signal frequencies.

From now on, let $\mathbf{Z}=\mathcal{DFT}(\mathbf{x})$, where
$\mathbf{x}\in\mathbb{R}^N$ is the $1\ s$ segment of $500\ Hz$ sEEG
data ($N=500$ is the sample size), and $\mathbf{Z}\in\mathbb{C}^F$ is its
Discrete Fourier Transform, sampling frequencies from $0\ Hz$ to $250\ Hz$ with 
$1\ Hz$ resolution ($F=251$ is the number of sampled frequencies).
We will also define index $n=0,1,...,F-1$ so that $|Z_n|^2$ is the power 
(squared amplitude) of frequency $f_n$ in the Fourier transform.

The following metrics were computed:
\begin{itemize}
    \item Spectral centroid: the mean of frequencies weighted by their power.
    Formally:
    \begin{equation}
        S_\text{centroid} = \frac
        {\sum_{n=0}^{F-1}{|Z_n(\mathbf{x})|^2f_n}}
        {\sum_{n=0}^{F-1}{|Z_n(\mathbf{x})|^2}}
    \end{equation}
    
    \item Spectral spread: the average dispersion of frequencies around the
    centroid. Formally:
    \begin{equation}
        S_\text{spread} = \sqrt{\frac
        {\sum_{n=0}^{F-1}{|Z_n(\mathbf{x})|^2(f_n-S_\text{centroid})^2}}
        {\sum_{n=0}^{F-1}{|Z_n(\mathbf{x})|^2}}}
    \end{equation}
     
    \item Spectral flatness: characterizes the tonality of the spectrum,
    intended as the tendency to present peaks in the spectrum opposed to flat
    noise. Formally:
    \begin{equation}
        S_\text{flatness}=
        \frac{\exp\left(\frac{1}{F+1}\sum_{n=0}^{F-1}\ln
        \left(|Z_n(\mathbf{x})|^2\right)+\epsilon\right)}
        {\frac{1}{F+1}\sum_{n=0}^{F-1}|Z_n(\mathbf{x})|^2}
    \end{equation}
    where $\epsilon=10^{-12}$ is a small constant to ensure that the logarithm
    is always defined. 
    
    \item Spectral entropy: captures the complexity of the spectrum.
    Sharp peaks will have low entropy while flat noise will produce high values.
    The definition is based on Shannon entropy, formally:
    \begin{equation}
        S_\text{entropy}=
        -\sum_{n=0}^{F-1}{p_n(\mathbf{x})\log_2{p_n(\mathbf{x})}}
    \end{equation}    
    where $p_n(\mathbf{x})=\frac{|Z_n(\mathbf{x})|^2} 
    {\sum_{n=0}^{F-1}|Z_n(\mathbf{x})|^2}$ and $\epsilon=10^{-12}$ is a small
    constant to ensure that the logarithm is always defined.
    Spectral entropy can be an indicator of anomalous behavior in both ictal
    and inter-ictal activity \cite{spectral-entropy}.
    
    \item Relative powerbands: integrated power between two frequencies, 
    normalized so that the sum of all powerbands at a given instant is equal to
    1.
    The ratio between powerbands has been proven to show anomalies during 
    epileptic events \cite{powerbands} and can be used in user-assisted methods
    to localize the SOZ \cite{bartolomei}.
    Formally, the relative powerband between frequencies $a$ and $b$ is
    defined as:
    \begin{equation}
    S_{\text{band}} = \frac{1}{C}
    \sum\limits_{f_n \in [a, b]} |Z_n(\mathbf{x})|^2
    \end{equation}
    where $C=\sum\limits_{n=0}^{F} |Z_n(\mathbf{x})|^2$ is a normalization factor
    forcing the sum of all powerbands to be equal to 1.
    The frequency intervals used in this study are reported in table
    \ref{tab:bands}.

\begin{table}[h]\label{tab:bands}
\centering
\begin{tabular}{lcc}
\hline
\textbf{Powerband} & \textbf{Frequency interval (Hz)} \\
\hline
Delta   & 0.5 -- 4 \\
Theta   & 4 -- 8 \\
Alpha   & 8 -- 12 \\
Beta    & 12 -- 30 \\
Gamma   & 30 -- 80 \\
Ripple  & 80 -- 250 \\
\hline
\end{tabular}
\caption{Relative powerbands considered in this study.}
\label{tab:powerbands}
\end{table}


\end{itemize}


\subsection{Variational Autoencoder (VAE)} \label{meth:vae}

A Variational AutoEncoder (VAE) is an artificial neural network architecture
\cite{vae-original}.
Like an autoencoder, it is trained to reconstruct its own input and is composed 
of an encoder and a decoder. 
Unlike an autoencoder, encoder and decoder are connected through a probabilistic
latent space.
The encoder maps each input point into a probabilistic distribution into a
lower-dimensional manifold known as latent space.
The decoder samples a random point from the encoded distribution, and
is trained to reconstruct its own input.
The random sampling in the latent space is known as "reparametrization trick".

The input vector $\mathbf{x}$ is transformed by the encoder in a pair of latent
vectors $(\mathbf{z_{\mu}},\mathbf{z_{\sigma}})$, while the latent vector
$\mathbf{z}\sim\mathcal{N}(\mathbf{z_{\mu}},\mathbf{z_{\sigma}})$ is sampled
from a normal distribution with mean $\mathbf{z_\mu}$ and variance
$\mathbf{z_{\sigma}}$.
The output vector is then computed by feeding $\mathbf(z)$ into a decoder.

Formally, a VAE has two objectives: 
\begin{enumerate}
    \item To maximize the likelihood of observed data $p_{\theta}(\mathbf{x})$
    under the distribution parameterized by the decoder parameters $\theta$.
    \item To learn an encoded distribution $q_{\phi}(\mathbf{z}|\mathbf{x})$ to
    approximate the posterior distribution $p_{\theta}(\mathbf{z}|\mathbf{x})$.
\end{enumerate}

This two goals can be achieved by maximizing the Evidence Lower BOund (ELBO):
\begin{align}
    L_{\theta, \phi}(\mathbf{x})&=\ln(p_{\theta}(\mathbf{x}))-
    D_{KL}(q_\phi(\mathbf{z}|\mathbf{x})\Vert p_{\theta}(\mathbf{z}|\mathbf{x}))
    \\
    &= \mathbb{E}_{\mathbf{z}\sim q_{\theta}(\mathbf{z}|\mathbf{x})}
    \left[p_\theta(\mathbf{x}|\mathbf{z})\right] - 
    \mathbb{E}_{\mathbf{z}\sim q_{\theta}(\mathbf{z}|\mathbf{x})}
    \left[\ln{\frac{q_\phi(\mathbf{z}|\mathbf{x})}{p_\theta(\mathbf{z})}}\right]
\end{align}

Since we assume that $p_\theta(\mathbf{x}|\mathbf{z})=
\mathcal{N}\left(\mathbf{D}_\theta(\mathbf{z}), \mathbb{I}\right)$, 
where $D_\theta(\mathbf{z})$ is the reconstructed input vector), 
the first term simplifies to the mean square error between the input and its
reconstructed error. 
The second term depends on the prior distribution $p_\theta(\mathbf{z})$ 
chosen for the latent space.
A common choice for the prior is the normal distribution 
$\mathcal{N}(\mathbf{0}, \mathbb{I})$. 
The reparametrization trick implies $q_\phi(\mathbf{z}|\mathbf{x}) = 
\mathcal{N}(\mathbf{\mu}_\phi(\mathbf{x}),
\mathbf{\sigma}^2_\phi(\mathbf{x}))$, therefore the ELBO is
optimizable as:

\begin{equation}    \label{ELBO-theoretical}
    L_{\theta, \phi}(\mathbf{x})=
    -\frac{1}{2}\mathbb{E}_{\mathbf{z}\sim q_{\theta}(\mathbf{z}|\mathbf{x})}
    \left[\Vert\mathbf{x} - \mathbf{D_\theta(\mathbf{z})}\Vert^2\right] - 
    \frac{1}{2} \sum_{i=1}^{d} \Big( \ln\mathbf{\sigma}_i^2(\mathbf{z})
    + \mathbf{\mu}_i^2(\mathbf{z}) + \mathbf{\sigma}_i^2(\mathbf{z}) - 1 \Big)
\end{equation}
where $d$ is the dimensionality of the latent space.

(!!! Esempio d'uso? Perché lo usiamo?)


\subsection{Gaussian Mixture Models (GMM)}  \label{meth:gmm}


\subsection{Negative log-likelihood}    \label{meth:neg_ll}


\subsection{Classification}         \label{meth:classifier}


\subsection{Hyperparameter Tuning}  \label{meth:opt}
