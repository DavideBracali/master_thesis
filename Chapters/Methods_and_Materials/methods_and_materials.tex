\pagestyle{fancy}
\fancyhead{}

\fancyheadoffset{0cm}
\renewcommand{\headrulewidth}{1pt} 
\renewcommand{\footrulewidth}{0pt}
\fancyhead[L]{{\thesection}}
\fancyhead[L]{\leftmark}

% \fancyhead[LO,LE]{\nouppercase\firstleftmark}
%\fancyhead[RO,RE]{\thesubsection\enspace\subsectiontitle}

\section{Methods and Materials}


\subsection{Dataset Description}

The dataset for this project includes sEEG recordings for 23 patients affected
by focal epilepsy.
Each patient provides one recording containing a seizure event (approximately 3
minutes) and at least 2 hours of resting state recordings, which we will refer 
as "wakes".
For 18 patients two wakes were available, while the other 5 patients only
included one wake.

Each recording consists of several time series, one for each contact (typically
150-200), sampled at 500 Hz.
Each contact is labeled as a letter, representing the electrode, followed by an
integer number referring to a segment of that electrode.
For example, contact "A1" represents the first segment of electrode "A".
An 30 seconds example of sEEG recordings is visible in Figure \ref{fig:exseeg}.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Immagini/exseeg.png}
    \caption{A 30 seconds extract of sEEG recordings during the onset of 
    a seizure on a representative patient. Only six contacts are shown for
    illustrative purposes.}
    \label{fig:exseeg}
\end{figure}

Beside the recordings a database was also provided, including the list of the
surgically removed brain regions.
All of the patients in this study underwent surgery, consisting in the 
thermocoagulation of the tissue surrounding the contacts labeled as SOZ.
For the majority of patients, the removal of these regions resulted in a drastic 
reduction of the symptoms of epilepsy. We will therefore consider this list as
the ground truth for this project.
The spatial coordinates of each contacts and onset time of the seizure 
(reported by doctors and corresponding to the first electrical symptoms)
were also included in the database.


\subsection{Pre-processing}

Pre-processing for this data consists in the application of notch filters to 
eliminate power line noise, and a bandpass Butterworth filter.

The notch filters were applied at $50\ Hz$, $100\ Hz$, $150\ Hz$ and $200\ Hz$
to remove the fundamental frequency and its fundamental harmonics.
A bandpass Butterworth 6th order zero-phase filter was applied on raw data to
isolate frequencies between $0.5\ Hz$, as very low frequencies
provide no useful information for this study \cite{eeg-book} and $249.5\ Hz$,
as frequencies higher than $250\ Hz$ would introduce aliasing in a $500\ Hz$ 
time series.

To lighten disk usage and computational time, signal was converted from $V$ to  
$\mu V$ and stored to \textit{float32}.


\subsection{The Brain Network} \label{meth:thebrainnetwork}

Network analysis was extensively used in this work: sEEG recordings are
localized, and provide the opportunity to investigate the functional
relationships between different regions of the brain.
Each brain region can be mapped into a node, while an appropriate similarity
measure between the recordings can be used to form weighted links.
Because sEEG recordings are time series, such network will also be dynamic,
enabling us to gain insights on the evolution of the network before, during and
after the seizure.

This approach is widely explored in literature \cite{eeg-book}, as epileptic
seizures are characterized by anomalous and synchronized electrical activity
\cite{WHO2016Epilepsy}.
In particular, in focal epileptic seizures the abnormal activity is initially
limited between the SOZ, and subsequently spreads to the entirety (or a vast
area) of the brain.
Functional brain networks are powerful tools to model such synchronization and
diffusion mechanisms.

Different choices are possible for the similarity measure, depending on the
focus of the study \cite{eeg-book}.
For this work mutual information was chosen to quantify the dependencies of
signal pairs, capturing linear and non-linear relationships to model the
dynamics of an epileptic seizure. Mutual information between two discrete
random variables $X\in\mathcal{X}$ and $Y\in\mathcal{Y}$ is defined as:
\begin{equation}
    \text{MI}(X;Y) = D_{KL}(P_{(X,Y)}\ \Vert\ P_X \otimes P_Y)
    = \sum_{x\in\mathcal{X}}\sum_{y\in\mathcal{Y}}
    {P_{(X,Y)}(x,y)\log{\frac{P_{(X,Y)}(x,y)}{P_{X}(x)\cdot P_{Y}(y)}}}
\end{equation}
where $D_{KL}$ is the Kullback–Leibler divergence, a statistical distance
between the joint distribution $(P_{(X,Y)}$ and the outer product of the
marginal distributions $P_X$ and $P_Y$. 
It is a non-negative and symmetric quantity that measures how much the joint
distribution of $(X,Y)$ differs from the product of the marginal distributions
of $X$ and $Y$ \cite{cover1991elements}. Hence, when two random variables
are independent, their mutual information is equal to zero.

Being a metric that quantifies the amount of shared information between two
variables, mutual information can be used to capture the linear and non-linear
relationships that model the synchronization dynamics typical of epilepsy.
For this reason, each sEEG channel was divided into $1\ s$ (or equivalently 500
time steps) non-overlapping time windows.
At a given instant, these segments represent the nodes of fully-connected,
weighted, undirected graph network.
The weight $w_{ij}$ of the link between two nodes $(i,j)$ is defined as:
\begin{equation}
    w_{ij}(t) = \frac{\text{MI}_{ij}(t)}{\text{MI}_{ii}}
\end{equation}
where $\text{MI}_{ij}(t)$ is the mutual information between signal i and j at time $t$.
It is normalized over the mutual information between identical signals $\text{MI}_{ii}$
so that weights always are between 0 and 1,
with $w_{ij} = 0$ indicating statistical
independence and $w_{ij} = 1$ corresponding to identical signals.

This process results in a dynamic set of networks with a temporal resolution of
$1\ s$. Higher temporal resolution could be possible, for example using a
sliding window instead of a non-overlapping one.
One must however keep in mind that, for each time frame, a different network has
to be constructed. The computation time to build and extract features will
therefore scale linearly with the number of time frames.


\subsection{Proposed Pipeline}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Immagini/pipeline.png}
    \caption{Proposed pipeline.}
    \label{fig:pipeline}
\end{figure}

The aim of this work is to develop an patient-independent algorithm to classify
which regions of the brain belong to the SOZ.
The proposed pipeline for this task is shown in Figure \ref{fig:pipeline}), and
can be summarized in the following steps:
\begin{itemize}
    \item Feature extraction: a set of 22 metrics is computed from pre-processed
    data, in order to extract meaningful features that characterize sEEG data.
    Details are provided in Section~\ref{meth:features}.
    \item Variational Autoencoder (VAE): features are used as input of a 
    VAE, which encodes data in a lower-dimensional representation.
    The model is trained on wakes to learn an encoded representation
    for the resting state of the brain.
    Details are provided in Section~\ref{meth:vae}
    \item Gaussian Mixture Model (GMM): a weighted sum of normal distributions
    is fitted on the latent space of the VAE, to learn the encoded
    distribution of features during resting state activity.
    Details are provided in Section~\ref{meth:gmm}
    \item Negative log-likelihood score: seizure data are projected into the
    trained latent space, where deviations from the learned resting-state
    distribution result in low likelihood values.
    The negative log-likelihood is computed at each time step and is considered
    as a measure of anomalous behavior.
    Details are provided in Section~\ref{meth:neg_ll}
    \item SOZ classification: we assume that the SOZ is constituted by 
    the contacts that begin to show anomalies a few seconds before the 
    generalized seizure. We use a deterministic algorithm on the negative 
    log-likelihood score to rank contacts based on an alarm score, proportional
    to the estimated probability of belonging to the SOZ.
    Finally, the computed alarm score is thresholded to obtain a binary 
    classification.
    Details are provided in Section~\ref{meth:classifier}
\end{itemize}


\subsection{Feature Extraction} \label{meth:features}

In order to distinguish between healthy and epileptogenic behavior in sEEG
recordings, a set of features must be computed at each time frame in order to
extract meaningful information from data.
In this study we extract a total of 22 local features to characterize each
contact during a seizure.
Local features can be grouped into three categories depending on their nature:
\begin{enumerate}
    \item Network features: they are computed from a graph functional network to
    explore pair-wise relationships between signals.
    \item Signal features: metrics that can be extracted directly from the sEEG
    pre-processed time series.
    \item Spectral features: metrics that characterize the short-time Fourier
    transform spectrogram of data.
\end{enumerate}
All metrics were extracted with a $1\ s$ temporal resolution, using
non-overlapping sliding windows for feature extractions.



\subsubsection{Network features}

Network features are metrics that are computed from the functional network
described in section \ref{meth:thebrainnetwork}.
Those quantities can both describe the global state of the brain network
(global features) and inspect the topological role of each individual node
within the network (local features).

Local metrics can provide insights on the individual role of each node in the
network.
Their main purpose in this study is to localize the portions of the brain who
play an important role during the seizure onset.
Global features and their evolution, on the other hand, can be important
to understand and analyze network dynamics before, during and after a seizure.

The following local measures were computed:
\begin{itemize}
    \item Strength: it's defined as the sum of the weights of the links
    associated with each node \cite{networks-weighted}.
    Formally strength of node $i$ reads as:
    \begin{equation}
        s_i = \sum_{j\neq i}{w_{ij}}
    \end{equation}
    Its distribution, and its statistical moments, can provide insights
    about the evolution of the network connectivity.
    High weight indicates that contact $i$ is, on average, high
    synchronization with all the other contacts.

    \item Clustering coefficient: for unweighted graphs, the clustering of a
    node is the fraction of possible triangles through that node that exist 
    \cite{networks-clustering2}.
    For weighted graphs, there are several ways to define clustering, and the
    one used here is defined as the geometric average of the triangle edge
    weights \cite{networks-clustering}.
    Intuitively, it quantifies the tendency of nodes to form highly
    interconnected communities in the network.

    \item Betweenness centrality: it is defined as the sum of the fraction of
    all shortest paths that pass through that node \cite{networks-betw}
    Formally betweenness
    centrality of node $i$ reads as:

     
    \begin{equation}
        g_i = \sum_{i\neq j\neq k}{\frac{\sigma_{jk}(i)}{\sigma_{jk}}}
    \end{equation}
    where $\sigma_{jk}$ is the number of shortest paths from node $j$ to node $k$
    and $\sigma_{jk}(i)$ is the number of shortest paths from node $j$ to node $k$
    that pass through node $i$.
    It is an important centrality measure that highlights nodes acting as
    bridges between different parts of the network.

    \item Eigenvector centrality: it measures the influence of a node in a
    network by considering not only the number of its direct connections but
    also the centrality of the nodes to which it is connected.
    This measure is computed from the eigenvector corresponding to the largest
    eigenvalue of the weighted adjacency matrix \cite{networks-eigen}:
    \begin{equation}
        W\mathbf{x} = \lambda_1\mathbf{x}
    \end{equation}
    where $W$ is the weighted adjacency matrix, $\lambda_1$ is its largest
    eigenvalue and $\mathbf{x}$ contains the eigenvector centrality of each node.
\end{itemize}

The global metrics used for this work are:
\begin{itemize}
    \item Assortativity: it measures the similarity of connections in the graph
    with respect to node strength.
    A positive value (assortative network) indicates that nodes attach
    preferentially to nodes with similar strength, while a negative value
    (disassortative network) corresponds to the opposite tendency
    \cite{networks-ass}.
    
    \item Average shortest path: the sum of shortest path lengths between all
    pairs of nodes:
    \begin{equation}
        a = \sum_{i\neq j}{\frac{d(i,j)}{n(n-1)}}
    \end{equation}
    where $d(i,j)$ is the shortest path distance between node $i$ and node $j$
    and $n$ is the total number of nodes.
    It can be generalized for weighted networks, using the inverse of the link
    weight as a distance between two nodes (so that higher weights are
    associated to shorter distances).

    \item The average along nodes of all the previously described local metrics.

\end{itemize}


\subsubsection{Signal features}

Signal features can be computed directly from the pre-processed sEEG time-series.
Such features were chosen to capture epileptogenic patterns that are directly
visible just looking at data, such as increased amplitude, sharp oscillations, 
or sudden transient spikes.

The following metrics were computed for each contact:
\begin{itemize}
    \item Mean: the arithmetic mean of the distribution of signal amplitude.
    
    \item Standard deviation: standard deviation of the distribution of signal
    amplitude, to capture increased signal variability during epileptogenic
    behavior.

    \item Skewness: measure of asymmetry the distribution of signal amplitude.
    It highlights deviations from symmetric signal activity.
    Has shown discriminative power in magnetoencephalography signals
    \cite{skewkurt}.

    \item Kurtosis coefficient: measures the tendency of the signal to produce
    extreme values, using a normal distribution as reference.
    High Kurtosis indicates frequent large excursions from the mean, which
    correspond to sharp transients and spikes commonly seen in epilepsy.
    Has shown discriminative power in magnetoencephalography signals
    \cite{skewkurt}.
    
    \item RMS (Root Mean Square): measures the overall signal power, equivalent 
    of the standard deviation assuming zero as mean.

    \item Peak-to-peak: maximum excursion between maximum and minimum.
    This metric is particularly sensitive to sudden spikes or sharp waves,
    indicators of epileptogenic activity.
     
    \item Maximum derivative: maximum absolute value of the difference between
    adjacent signal samples, represents the maximum steepness of the signal.
    
    \item Highuci fractal dimension: approximate box-counting dimension of the
    fractal that underlines the time-series.
    Captures signal complexity and self-similarity, and can be a powerful
    discriminant in identifying seizure onset \cite{higuchi}.
\end{itemize}


\subsubsection{Spectral features} \label{meth:features:spectral}

Spectral features are derived from the spectrogram of pre-processed signals.
The spectrogram, a time-dependent representation of the frequency domain of
signal, was computed using a short-time Fourier transform.
It consists in the sequencing of data using $1\ s $ non-overlapping time windows
and the application of a Fourier transform on those windows, providing a dynamic
analysis of the signal frequencies.

From now on, let $\mathbf{Z}=\mathcal{DFT}(\mathbf{x})$, where
$\mathbf{x}\in\mathbb{R}^N$ is the $1\ s$ segment of $500\ Hz$ sEEG
data ($N=500$ is the sample size), and $\mathbf{Z}\in\mathbb{C}^F$ is its
Discrete Fourier Transform, sampling frequencies from $0\ Hz$ to $250\ Hz$ with 
$1\ Hz$ resolution ($F=251$ is the number of sampled frequencies).
We will also define index $n=0,1,...,F-1$ so that $|Z_n|^2$ is the power 
(squared amplitude) of frequency $f_n$ in the Fourier transform.

The following metrics were computed:
\begin{itemize}
    \item Spectral centroid: the mean of frequencies weighted by their power.
    Formally:
    \begin{equation}
        s_\text{centroid} = \frac
        {\sum_{n=0}^{F-1}{|Z_n(\mathbf{x})|^2f_n}}
        {\sum_{n=0}^{F-1}{|Z_n(\mathbf{x})|^2}}
    \end{equation}
    
    \item Spectral spread: the average dispersion of frequencies around the
    centroid. Formally:
    \begin{equation}
        s_\text{spread} = \sqrt{\frac
        {\sum_{n=0}^{F-1}{|Z_n(\mathbf{x})|^2(f_n-s_\text{centroid})^2}}
        {\sum_{n=0}^{F-1}{|Z_n(\mathbf{x})|^2}}}
    \end{equation}
     
    \item Spectral flatness: characterizes the tonality of the spectrum,
    intended as the tendency to present peaks in the spectrum opposed to flat
    noise. Formally:
    \begin{equation}
        s_\text{flatness}=
        \frac{\exp\left(\frac{1}{F}\sum_{n=0}^{F-1}\ln
        \left(|Z_n(\mathbf{x})|^2\right)+\epsilon\right)}
        {\frac{1}{F}\sum_{n=0}^{F-1}|Z_n(\mathbf{x})|^2}
    \end{equation}
    where $\epsilon=10^{-12}$ is a small constant to ensure that the logarithm
    is always defined. 
    
    \item Spectral entropy: captures the complexity of the spectrum.
    Sharp peaks will have low entropy while flat noise will produce high values.
    The definition is based on Shannon entropy, formally:
    \begin{equation}
        s_\text{entropy}=
        -\sum_{n=0}^{F-1}{p_n(\mathbf{x})\log_2{p_n(\mathbf{x})}}
    \end{equation}    
    where $p_n(\mathbf{x})=\frac{|Z_n(\mathbf{x})|^2} 
    {\sum_{n=0}^{F-1}|Z_n(\mathbf{x})|^2}$ and $\epsilon=10^{-12}$ is a small
    constant to ensure that the logarithm is always defined.
    Spectral entropy can be an indicator of anomalous behavior in both ictal
    and inter-ictal activity \cite{spectral-entropy}.
    
    \item Relative powerbands: integrated power between two frequencies, 
    normalized so that the sum of all powerbands at a given instant is equal to
    1.
    The ratio between powerbands has been proven to show anomalies during 
    epileptic events \cite{powerbands} and can be used in user-assisted methods
    to localize the SOZ \cite{bartolomei}.
    Formally, the relative powerband between frequencies $a$ and $b$ is
    defined as:
    \begin{equation}
    s_{\text{band}} = \frac{1}{C}
    \sum\limits_{f_n \in [a, b]} |Z_n(\mathbf{x})|^2
    \end{equation}
    where $C=\sum\limits_{n=0}^{F} |Z_n(\mathbf{x})|^2$ is a normalization factor
    forcing the sum of all powerbands to be equal to 1.
    The frequency intervals used in this study are reported in table
    \ref{tab:bands}.

\begin{table}[h]
\centering
\begin{tabular}{lcc}
\hline
\textbf{Powerband} & \textbf{Frequency interval (Hz)} \\
\hline
Delta   & 0.5 -- 4 \\
Theta   & 4 -- 8 \\
Alpha   & 8 -- 12 \\
Beta    & 12 -- 30 \\
Gamma   & 30 -- 80 \\
Ripple  & 80 -- 250 \\
\hline
\end{tabular}
\caption{Relative powerbands considered in this study.}
\label{tab:bands}
\end{table}


\end{itemize}


\subsection{Variational Autoencoder (VAE)} \label{meth:vae}

A Variational AutoEncoder (VAE) is an artificial neural network architecture
\cite{vae-original}.
Like an autoencoder, it is trained to reconstruct its own input and is composed 
of an encoder and a decoder. 
Unlike an autoencoder, encoder and decoder are connected through a probabilistic
latent space.
The encoder maps each input point into a probabilistic distribution into a
lower-dimensional manifold known as latent space.
The model then samples a random point from the encoded distribution, and passes
it to an autoencoder trained to reconstruct the initial input.

The input vector $\mathbf{x}$ is transformed by the encoder in a pair of latent
vectors $(\mu({\mathbf{x})},\sigma({\mathbf{x}}))$, while the latent vector
$\mathbf{z}\sim\mathcal{N}(\mu(\mathbf{x}),\sigma^2(\mathbf{x}))$ is sampled
from a normal distribution with mean $\mu(\mathbf{x})$ and variance
$\sigma^2(\mathbf{x})$.
The output vector is then computed by feeding $\mathbf{z}$ into a decoder.
To make sampling differentiable the latent vector is sampled as:
\begin{equation}
    \mathbf{z}=\mu(\mathbf{x}) + \sigma(\mathbf{x})\odot\epsilon
    \;\;\;\;\;\;\;\;\;\;\;\;\epsilon\sim\mathcal{N}(0,1)
\end{equation}
This is known as "reparametrization trick".

Formally, a VAE has two objectives: 
\begin{enumerate}
    \item To maximize the likelihood of observed data $p_{\theta}(\mathbf{x})$
    under the distribution parameterized by the decoder parameters $\theta$.
    \item To learn an encoded distribution $q_{\phi}(\mathbf{z}|\mathbf{x})$ to
    approximate the posterior distribution $p_{\theta}(\mathbf{z}|\mathbf{x})$.
\end{enumerate}

This two goals can be achieved by maximizing the Evidence Lower BOund (ELBO):
\begin{align}
L_{\theta, \phi}(\mathbf{x}) &=
    \ln p_{\theta}(\mathbf{x}) 
    - D_{\text{KL}}\Big( q_\phi(\mathbf{z}|\mathbf{x}) \,\Vert\, p_{\theta}(\mathbf{z}|\mathbf{x}) \Big)
    =\\
    &= \mathbb{E}_{\mathbf{z}\sim q_\phi(\mathbf{z}|\mathbf{x})}
    \left[\ln p_\theta(\mathbf{x}|\mathbf{z})\right] - 
    \mathbb{E}_{\mathbf{z}\sim q_{\phi}(\mathbf{z}|\mathbf{x})}
    \left[\ln{\frac{q_\phi(\mathbf{z}|\mathbf{x})}{p_\theta(\mathbf{z})}}\right]
\end{align}
where $D_{KL}$ is the Kullback–Leibler (KL) divergence.

Since we assume that $p_\theta(\mathbf{x}|\mathbf{z})=
\mathcal{N}\left(\mathbf{D}_\theta(\mathbf{z}), \mathbb{I}\right)$, 
where $D_\theta(\mathbf{z})$ is the reconstructed input vector, 
the first term simplifies to the mean square error between the input and its
reconstructed error, plus an additive constant. 
The second term depends on the prior distribution $p_\theta(\mathbf{z})$ 
chosen for the latent space.
A common choice for the prior is the normal distribution 
$\mathcal{N}(\mathbf{0}, \mathbb{I})$. 
The reparametrization trick implies $q_\phi(\mathbf{z}|\mathbf{x}) = 
\mathcal{N}(\mathbf{\mu}_\phi(\mathbf{x}),
\mathbf{\sigma}^2_\phi(\mathbf{x}))$, therefore the ELBO is
optimizable as:

\begin{equation}    \label{ELBO-theoretical}
    L_{\theta, \phi}(\mathbf{x})=
    -\frac{1}{2}\mathbb{E}_{\mathbf{z}\sim q_\phi(\mathbf{z}|\mathbf{x})}
    \left[\Vert\mathbf{x} - \mathbf{D_\theta(\mathbf{z})}\Vert^2\right] - 
    \frac{1}{2} \sum_{i=1}^{d} \Big( \ln\mathbf{\sigma}_{\phi i}^2(\mathbf{x})
    + \mathbf{\mu}_{\phi i}^2(\mathbf{x}) +
    \mathbf{\sigma}_{\phi i}^2(\mathbf{x}) - 1\Big)
\end{equation}
where $d$ is the dimensionality of the latent space.

The loss function to minimize during training is therefore composed of the
sum of the Mean Squared Error (MSE) between input and output and the KL
divergence between the prior and the approximate posterior on the latent space.
A popular variation of such loss includes a scalar term $\beta(t)$,
optionally dependent on the training epoch $t$, to regulate the relative weight
of the KL term with respect to the MSE term \cite{vae-beta-original}.
We also decide to divide the KL term by the latent dimension $d$, so that
the relative weight of the KL term is controlled solely by $\beta(t)$ and does 
not depend on the dimensionality of the latent space.
Therefore the loss function implemented in this process is:
\begin{equation}
    \mathcal{L}_{\theta, \phi}(\mathbf{x},t)=
    \frac{1}{2}\mathbb{E}_{\mathbf{z}\sim q_{\phi}(\mathbf{z}|\mathbf{x})}
    \left[\Vert\mathbf{x} - \mathbf{D_\theta(\mathbf{z})}\Vert^2\right] + 
    \frac{\beta(t)}{2} \sum_{i=1}^{d} \Big( \ln\mathbf{\sigma}_{\phi i}^2
    (\mathbf{x})
    + \mathbf{\mu}_{\phi i}^2(\mathbf{x}) +
    \mathbf{\sigma}_{\phi i}^2(\mathbf{x}) - 1 \Big)
\end{equation}

As shown by literature \cite{vae-beta}, if $\beta(t)$ is an increasing function 
of the training epoch (a process known as "warm-up"), training becomes more 
stable as posterior collapse (i.e. when the approximate posterior collapses to 
the prior) is less likely to happen.

In this work, a VAE is trained on the resting-state recordings of multiple
patients.
Because each patient provides training data of different length (from 2 to 7
hours), we sample a fixed numbers of data points from the resting-state
recordings of each patient.
The sampling is uniform over time, ensuring that the selected points are spread
across the entire recording rather than clustered in a short temporal segment.
Validation split is set as $10\%$, as this portion of training data is used to
monitor early-stopping.

The size of the input and output layer is 22, because it is the number of
extracted features (Section \ref{meth:features}).
All features are standardized before entering the VAE as input.
The encoder architecture consists in an arbitrary number of hidden layers,
fully connected through ReLU activation functions.
The number of neurons of each hidden layer decreases linearly from 22 to the
latent dimension.
The encoder terminates into two distinct linear layers that return mean
$\mu({\mathbf{x})}$ and log-variance $\ln\sigma({\mathbf{x})}$ of the encoded
distribution.
After sampling
$\mathbf{z}\sim\mathcal{N}(\mu({\mathbf{x})},\sigma({\mathbf{x}}))$ the latent
vector is reconstructed by the decoder, with a specular architecture of hidden
layers with respect to the encoder and linear activation functions.

Training is executed in two steps:
\begin{enumerate}
    \item The first training phase sets $\beta(t)=0$, so that the model can
    learn a meaningful encoding that enables for efficient 
    reconstruction. Early-stopping is set with a patience of 5 epochs and a
    $1\%$ tolerance.
    \item The second training phase sets
    $\beta(t)=\beta_{\text{max}}\min{(t/\beta_{\text{warmup}}, 1)}$, 
    a linear increasing function that reaches $\beta_{\text{max}}$ after 
    $\beta_{\text{warmup}}$ iterations.
    After the first initial phase, now the model is forced to sacrifice
    reconstruction to obtain a smooth and interpolable latent space
    distribution. Early-stopping is set with a patience of 5 epochs and a
    $1\%$ tolerance.
\end{enumerate}

This architecture depends on 5 hyperparameters, that will be optimized as
explained in Section \ref{meth:opt}:
\begin{itemize}
    \item The learning rate $\eta$.
    \item The number of hidden layers $L$.
    \item The latent dimension $d$.
    \item $\beta_{\text{warmup}}$
    \item $\beta_{\text{max}}$
\end{itemize}


In this work we implement a VAE model because we are interested in a
lower-dimensional representation of data that can extract complex non-linear
patterns while keeping a smooth and interpolable structure.
Other models could be used for the same task, such as linear alternatives like 
Principal Components Analysis (PCA) and Partial Least Squares Regression (PLS).
The VAE framework could also be extended to include a supervised term in the
loss function to encourage separation between epileptogenic and healthy contacts
during training.
These alternatives were not explored in this study, but could be worth
investigating in future research.

\subsection{Gaussian Mixture Models (GMM)}  \label{meth:gmm}

A Gaussian Mixture Model (GMM) is a probabilistic model that assumes all the data
points are generated from a mixture of a finite number of Gaussian distributions
with unknown parameters.
A GMM assumes that data is generated by a weighted sum of normal distributions:
\begin{equation}
    p_\psi(\mathbf{x}) = \sum_{k=1}^{N_c}\pi_k\mathcal{N}
    (\mathbf{x} \mid \mu_{\psi k}, \Sigma_{\psi k})
\end{equation}
where $N_c$ is the fixed number of Gaussian components.
A GMM model is fit on data $\{\mathbf{x}_i\}_{i=1}^N$ maximizing the
log-likelihood of the model $p_\psi(\mathbf{x})$ on the data:

\begin{equation}
    \hat{\psi} = \text{arg}\max_{\psi}\sum_{i=1}^N
    \ln\left(\sum_{k=1}^{N_c}\pi_k\mathcal{N}
    (\mathbf{x}_i \mid \mu_{\psi k}, \Sigma_{\psi k})\right)
\end{equation}
However, this equation does not have a closed form solution.
To address this problem,
an Expectation-Maximization (EM) algorithm is used to maximize the likelihood 
(Bishop, 2006, Cap.~9) \cite{bishop2006}.

A GMM with an arbitrary number of components can be used to fit more complex
distributions, and is particularly efficient in moderate-dimensional spaces
when confronted against non-parametric methods.
GMM assumes an underlying distribution for data, therefore the model does not
have to estimate the density of data points like Kernel Density Estimators or
k-Nearest Neighbors.
With a moderate number of dimensions, GMM will train significantly faster than
other density estimators.

In this work we use a GMM with a fixed number of components $N_c$ to learn the
distribution of resting-state activity in the latent space of the VAE described
in Section \ref{meth:vae}:
resting-state features are encoded in a lower-dimensional latent space, and a
GMM fits the learned distribution of these latent representations, capturing the
regions most frequently visited by healthy brain activity.
The number of components $N_c$ will be optimized as explained in Section 
\ref{meth:opt}.


\subsection{Negative log-likelihood}    \label{meth:neg_ll}

As mentioned in Section \ref{meth:vae} and Section \ref{meth:gmm},
the VAE is trained on resting-state recordings and a GMM is fit on the latent
space to learn the encoded distribution of the healthy state of the brain.
When features extracted from seizure recordings are fed into the trained VAE,
we expect that:
\begin{itemize}
    \item Healthy behavior will be encoded into regions of the latent space that
    were frequently visited during training;
    \item Anomalous behavior will be encoded into regions of the latent space
    that were rarely or never visited during training.
\end{itemize}

To estimate how well seizure data is explained by the training distribution,
we score the likelihood $\mathcal{L}$ of the GMM model for each contact at every
step of the seizure.
High likelihood values indicate conformity with the training distribution, while
low values are indicative of deviations from the training distribution.
We therefore use, for each seizure vector $\mathbf{x}_c(t)$ ($c$ indicates the
contact while $t$ is the time index)
the negative log-likelihood as a quantitative indicator for anomalous behavior
with respect to the resting-state:
\begin{equation}
    \text{NLL}_c(t) = -\log_{10}\mathcal{L}\left(\psi|\mathbf{x}_c(t)\right)    
\end{equation}
where $\psi$ represents the GMM parameters, $t$ indicates the time step.

The aim of this work is to classify the contacts belonging to the SOZ.
By definition, a focal epileptic feature manifests a few seconds earlier in the
SOZ, and subsequently spreads to a vaster area of the brain.
The negative log-likelihood can therefore be used to highlight anomalous 
patterns before the general seizure, as
it serves as an universal indicator for a deviation in one or more features from
its expected value assuming resting-state activity.

\subsection{Classification}         \label{meth:classifier}

As described in Section \ref{meth:neg_ll} we use the negative log-likelihood to
describe, for each contact at each time step, deviations from resting-state
activity during seizures.
To decide which contact constitute the SOZ, we use a deterministic classifier
that evaluates the time-series of the negative log-likelihood of each contact 
during the seizure.
In particular, the classifier evaluates log-likelihood in the interval
$[t_{\text{onset}} - 10\ s, t_{\text{onset}} +20\ s]$, where $t_{\text{onset}}$
is the reported time for the first electrical symptoms.
The lower bound of this interval is justified by the fact that in some patients
anomalies begin a few seconds before the reported onset, and the upper bound is
chosen to capture the early phase of the seizure while limiting the influence of
the general diffusion to a vaster area of the brain.

First, negative log-likelihood is smoothed in time with the application of a 
Gaussian 1-dimensional filter.
This operation is performed to remove noise from the time-series and to 
ensure that only anomalies that are sustained for several seconds are detected.
The filter application consists in the convolution of the negative
log-likelihood and a Gaussian kernel with zero mean:
\begin{equation}
    \text{sNLL}_c(t)=\text{NLL}_c(t)\circledast B(t)
    \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;
    B(t)=\frac{1}{\sqrt{2\pi}\sigma_{filt}}
    \exp{\left(-\frac{t^2}{2\sigma_{filt}^2}\right)}
\end{equation}
where $\sigma_{filt}$ was arbitrarily set to 3 based on the seizures of 
the patients of this dataset.
Anomalies sustained for more than $3\ s$ will be preserved, while shorter 
anomalies will be smoothed out.

Then two types of weights are applied:
\begin{itemize}
    \item A time weight that increases linearly for $t\leq t_{onset}$
    and decreases exponentially for $t > t_{\text{onset}}$:
    \begin{equation}
    f_{t_C}(t) =
    \begin{cases}
    \dfrac{t - t_{\text{onset}}+10}{10},
    & t_{\text{onset}}-10\leq t \leq t_{\text{onset}} \\[2mm]
    \exp\!\Big(-\dfrac{(t-t_{\text{onset}})/20}{t_C}\Big),
    & t_{\text{onset}} < t \leq t_{\text{onset}} + 20
    \end{cases}
    \end{equation}
    where the characteristic time $t_C>0$ is a scalar constant that controls the
    decay of the decreasing exponential.
    \item A betweenness centrality
    \footnote{The choice of betweenness centrality as a weight for the
    log-likelihood is justified by a ranking of the most discriminative
    features in a subset of 4 patients.
    As will be explained in Section \ref{res:anal:importance}, betweenness centrality
    is the only significantly significant feature for distinguishing between
    epileptogenic and healthy contacts during a seizure.}
    weight based on its Z-score from the 
    training distribution:
    \begin{equation}
        z_c(t) = \left|\frac{b_c(t)-\mu_{\text{train},c}}
        {\sigma_{\text{train},c}}\right|
    \end{equation}
    where $\mu_{\text{train},c}$ is the mean of the betweenness centrality of
    contact $c$ during training and $\sigma_{\text{train},c}$ is the standard
    deviation of contact $c$ during training.
    If $\sigma_{\text{train},c}=0$, the global standard deviation between all
    contacts is used as a substitute.
\end{itemize}

An initial alarm score is computed as:
\begin{equation}
    A^*_c(t_C,\alpha) = \sum_{t = t_{\text{onset}} - 10}^{t_{\text{onset}} + 20} 
    \left( \alpha\, f_{t_C}(t) + (1-\alpha)\, z_c(t) \right) \text{sNLL}_c(t)
\end{equation}
where $\alpha\in[0,1]$ is a scalar constant that regulates the relative 
influence of the two weights.

Because brain areas associated to the SOZ tend to thermocoagulated
abundantly (to avoid having to re-operate), we compute
a final alarm score that keeps into consideration the alarm scores of the 
surrounding contacts.
Let $\mathbf{s}_c=(s_{c,x}, s_{c,y}, s_{c,z})$ be the spatial coordinates of 
contact $c$.
Then, the final alarm score will add the initial score of all the other contacts
$c'\neq c$ weighted by their distance.
Formally:
\begin{equation}
    A_c(t_C,\alpha,k) = A^*_c + \sum_{c'\neq c}
    \text{dist}(\mathbf{s}_c,\mathbf{s}_{c'})^k A^*_{c'}
\end{equation}
where
$\text{dist}(\mathbf{s}_c, \mathbf{s}_{c'})=
\Vert\mathbf{s}_c-\mathbf{s}_{c'}\Vert_2$
and $k<0$ is a negative scalar constant that regulates the proximity scale.

The alarm score is then normalized so that the sum of the alarm score on all
contacts in a patient is equal to the number of contacts $C$.
Equivalently, the average alarm score for the contact of a patient is always 
equal to 1.
Normalization allows to compare alarm scores of different patients with a
different number of contacts.
The normalized alarm score is:
\begin{equation}
    a_c(t_C,\alpha,k) = C\frac{A_c(t_C,\alpha,k)}{\sum_c A_c(t_C,\alpha,k)}
\end{equation}

Finally, to obtain a binary classification, $a_c$ is thresholded by a threshold
$\tau$.
If $a_c\geq\tau$ then the contact is labeled as SOZ, if $a_c<\tau$ the contact
will be labeled as healthy:
\begin{equation}
    y_c(t_C,\alpha,k,\tau) = \theta(a_c(t_C,\alpha,k)-\tau)
\end{equation}
where $\theta(\cdot)$ is the Heavyside step function.

The classification therefore depends on 4 hyperparameters:
\begin{itemize}
    \item The characteristic time $t_C$;
    \item The relative ratio of weights $\alpha$;
    \item The negative exponent that regulates proximity $k$;
    \item The threshold $\tau$.
\end{itemize}


\subsection{Hyperparameter Tuning}  \label{meth:opt}

The proposed pipeline for this study depends on a total of 10 hyperparameters:
\begin{itemize}
    \item VAE and GMM parameters:
    \begin{itemize}
        \item The learning rate $\eta$;
        \item The number of hidden layers $L$;
        \item The latent dimension $d$;
        \item $\beta_{\text{warmup}}$;
        \item $\beta_{\text{max}}$;
        \item The number of Gaussian components $N_c$;
    \end{itemize}
    \item Classifier parameters:
    \begin{itemize}
        \item The characteristic time $t_C$;
        \item The relative ratio of weights $\alpha$;
        \item The negative exponent that regulates proximity $k$;
        \item The threshold $\tau$.
    \end{itemize}
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Immagini/patients.png}
    \caption{Dataset partitioning strategy}
    \label{fig:patients}
\end{figure}

To find the optimal values for these constants a Leave-One-Out Cross Validation
(LOOCV) approach was employed.
The 23 patients that constitute the dataset are divided based on the following
logic (Figure \ref{fig:patients}):
\begin{itemize}
    \item 4 patients were held out as an independent test set.
    \item 19 patients were used for training:
    \begin{itemize}
        \item 4 out of 19 patients are excluded from validation, as their
        seizures have already been analyzed in order to define the classification
        strategy discussed in Section \ref{meth:classifier}.
        These patients are only used to train the model, but are not evaluated
        as their results would be overestimated because of the choice of the
        classification strategy.
        \item 15 out of 19 patients were used both for training and validation.
    \end{itemize}
\end{itemize}

This approach results in 15 training folds.
Each fold is composed of 18 training patients, and one validation patient.
This strategy estimates how a certain set of hyperparameters, optimized on 18
patients, would generalize on a new patient that was never seen before by the
model.

Different training folds will generally produce different parameters. 
For this reason, after performing LOOCV, the model is trained one final time
using all 19 patients as training set to obtain a final configuration of
parameters, and evaluates those parameters on the independent 4 patients test
set.

Hyperparameter tuning was performed using the Optuna optimization framework
\cite{optuna}, performing parameters search using a combination of algorithms
based on Bayesian optimization.
At each iteration a new promising configuration of candidate parameters is
proposed, based on the previous values of the target to maximize.
To monitor overfitting during LOOCV, validation performance are evaluated for
each best-so-far configuration of parameters.
If training continues for many iterations, the model may overfit the training
data, leading to overly optimistic training performance but poor generalization 
to unseen subjects.
This is important because when the model is trained for test evaluation no
validation set is available, as all patients are used for training.
For this reason, the final training is stopped at the iteration corresponding to
the maximum median validation target across LOOCV folds, selecting the best
compromise between optimization and generalization.

To limit computation time and to reduce the hyperparameters space, 
thus simplifying the optimization problem, we decided to adopt a 2-step strategy.
First we optimize the VAE and GMM parameters, maximizing an intermediate target
function that produces an efficient encoding and fit on the latent space.
Then, once the VAE and GMM architecture is fixed, we optimize the classifier by
maximizing binary evaluation metrics.

\subsubsection{VAE and GMM}     \label{meth:opt:vaegmm}

The first optimization step aims to find optimal parameters for the model
architecture.
This step depends on 6 hyperparameters: $\eta$, $L$, $d$,
$\beta_{\text{warmup}}$, $\beta_{\text{max}}$, $N_c$.

The optimization logic is to select a model architecture so that, during a 
seizure, contacts associated with the SOZ exhibit low likelihood values
more often than non-SOZ contacts (refer to Section \ref{meth:neg_ll} for the
negative log-likelihood computation).
This is justified by the fact that the classifier (discussed in Section
\ref{meth:classifier}) will use the negative-log likelihood ($NLL$) to compute the
alarm score.
We therefore perform a one-sided Mann - Whitney U test \cite{mw} on the negative
log-likelihood distributions of SOZ and non-SOZ contacts in the 
$[t_{\text{onset}} - 10, t_{\text{onset}} + 20]$ interval during the seizure.
The rank biserial correlation, defined as:
\begin{equation}
    r = \frac{2U}{n_1n_2}-1
\end{equation}
where $U$ is the test statistic in the Mann Whitney test and $n_1$ and $n_2$
represent the sample size of the two distributions.
$r=+1$ implies that all SOZ contacts have higher NLL than non-SOZ contacts,
while $r=-1$ represents the opposite situation.
High values of $r$ indicate that the model is able to assign low likelihood
values to SOZ contacts more often, effectively recognizing the anomalous behavior
of such channels.
For this reason the rank biserial correlation $r$ serves as the objective
function to be maximized in this step.

\subsubsection{Classifier}  \label{meth:opt:classifier}

The second part of the optimization process aims to find optimal parameters for
the following hyperparameters: $t_C$, $\alpha$, $k$.
The threshold $\tau$ is handled differently than the other parameters.
For each candidate configuration of 
the classifier produces a continuous alarm score. Rather than treating 
$\tau$ as an additional hyperparameter to be sampled during optimization, we 
determine it deterministically by selecting the value that 
maximizes the overall performance across patients.

In this step we maximize a composition of binary classification metrics.
In future sections we will refer to the following metric as "score":
\begin{equation}    \label{eq:score}
    S = \frac{PPV+SPC}{2}\cdot f(TP)
\end{equation}
where:
\begin{itemize}
    \item $PPV$ (Positive Predicted Values) is the precision, defined as the 
    fraction of predicted positive values that are labeled as positives:
    \begin{equation}
        PPV = \frac{TP}{TP + FP}
    \end{equation}
    where $TP$ is the number of True Positives and $FP$ is the number of False
    Positives.
    
    \item $SPC$ is the specificity, defined as the fraction of labeled negative
    values that are correctly classified as negative:
    \begin{equation}
        SPC = \frac{TN}{TN + FP}
    \end{equation}
    where $TN$ is the number of True Negatives and $FP$ is the number of False
    Positives.

    \item $f(TP)$ is a penalty term in the
    form of a sigmoid function of the number of True Positives:
    \begin{equation}
        f(TP)=\frac{1}{1+\exp{\left(-(TP-2)\right)}}
    \end{equation}
    This term is introduced because optimizing only precision and specificity 
    often favors degenerate solutions in which very few contacts are predicted
    as positive.
    In such cases, precision and specificity are both artificially high
    despite the model detecting only a minimal number of true positives.
    $f(TP)$ heavily penalizes configurations with less than 3 True Positives,
    and quickly saturates to 1 afterwards:
    \begin{itemize}
        \item $f(1)\approx0.27$
        \item $f(2)=0.5$
        \item $f(3)\approx0.73$
        \item $f(4)\approx0.88$
        \item $f(5)\approx0.95$
        \item $f(6)\approx0.98$
    \end{itemize}
\end{itemize}

After computing the continuous alarm scores $a^{(p)}_c(t_C,\alpha,k)$ for each
contact on patient $p$ on the training set ${p_1,p_2,...}$, the score $S_p$
depends on the threshold $\tau$ to apply.
The threshold is selected from a discrete grid of uniformly spaced values in the
interval, maximizing the median score on all training patients:
\begin{equation}
    \tau_{best}=\text{arg}\max_{\tau\in\mathcal{T}}
    \left(\text{median}_{p\in\{p_1,p_2,...\}}S_p(\tau,t_C,\alpha,k)\right)
\end{equation}
where $\mathcal{T}=\{1.0,1.1,...,2.0\}$.

Finally the objective function to maximize during training is:

\begin{equation}
    \text{median}_{p\in\{p_1,p_2,...\}}S_p(\tau_{best},t_C,\alpha,k)
\end{equation}

Notice how none of the terms that constitute the score (equation \ref{eq:score})
depends on the number of False Negatives ($FN$).
This choice is motivated by the fact that, for this work, we consider the list 
of thermocoagulated contacts as ground truth.
The clinical decision on which areas of the brain must be removed is taken
based on the observations of many seizures, however the dataset for this work
only includes one seizure.
This implies that, during the seizure that we observe, some thermocoagulated
contacts may not show anomalies, as seizure dynamics can vary substantially.
With this choice of the objective function we train a model to recognize only 
the contacts that show clear anomalous patterns during the observed seizures
without penalizing the model if some thermocoagulated contacts are classified
as negative.

\subsection{Evaluation}

To quantify the classification performance we compute a set of evaluation
metrics on both validation and test results. 
We also compare the model to:
\begin{itemize}
    \item A random classifier with a probability of $0.5$.
    \item An automated version\footnote{
    Bartolomei et al. method depends on patient specific parameters to be set by
    an epileptologist, therefore the original algorithm cannot be implemented.
    An automated version of the algorithm has been employed for this study as
    explained in Section \ref{app:bartolomei} of the appendix.
    } of the Epileptogenicity Index (EI) proposed by
    Bartolomei et al. \cite{bartolomei}.
\end{itemize}
Let $\pi_p$ represent the labeled positive ratio for each patient. 
The following metrics were computed for evaluation:
\begin{itemize}
    \item Precision (PPV):
    \begin{equation}
        PPV = \frac{TP}{TP + FP}
    \end{equation}
    A random classifier will return an expected value of $PPV_{rand}=\pi_p$. 
 
    \item Specificity (SPC):
    \begin{equation}
        SPC = \frac{TN}{TN + FP}
    \end{equation}
    A random classifier will return an expected value of $SPC_{rand}=0.5$. 
    
    \item Balanced Accuracy (BA):
    \begin{equation}
        BA = \frac{TPR + SPC}{2}=
        \frac{1}{2}\frac{TP}{TP + FN}+\frac{1}{2}\frac{TN}{TN + FP}
    \end{equation} 
    A random classifier will return an expected value of $BA=0.5$.

    \item Area Under Curve of the Receiver Operating Characteristic (ROC - AUC):
    the ROC curve plots the True Positives rate and the False Positives Rate
    as the threshold varies. 
    The corresponding AUC provides a threshold-independent measure of the 
    model’s discriminative ability, and is directly related to the
    probability that the model produces an higher alarm score $a_c$
    (Section \ref{meth:classifier}) for a randomly selected positive as opposed 
    to a randomly selected negative \cite{rocauc}. 
    A random classifier will return an expected value of $BA=0.5$.
\end{itemize}














